---
layout: post
title: R-CNN 阅读笔记
category: DeepLearning
tags: "Deep-Learning"
---
# R-CNN
[R-CNN,SPP-NET, Fast-R-CNN,Faster-R-CNN, YOLO, SSD系列深度学习检测方法梳理](http://www.cnblogs.com/venus024/p/5717766.html)
## 1. R-CNN: Rich feature hierarchies for accurate object detection and semantic segmentation
> Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a  significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features.
* 用higt-capacity convolutional neural network(CNNs) 去实现定位;
* 当labels缺乏的时候，可以supervised pre-training for an auxiliary task.

> Unlike image classification, detection requires localizing (likely many) objects within an image.
* Detection需要定位（locate），而Classification不需要。

One approach frames localization as a regression problem, however, it is proved that this strategy may not fare well in practice. An alternative is to build a sliding-window detector. CNNs have been used in this way for at least two decades, typically on constrained object categories.
文中作者提到的方法如下
> In stead, we solve the CNN localization problem by operating within the "recognition using regions" paradigm, as argued for by Gu *et al.*. At test-time, our method generates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs. We use a simple technique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. Figure 1 presents an overview of out method and highlights some of our results. Since our system combines region proposals with CNNs, we dub the method R-CNN: Regions with CNN.
* 训练的时候从每张图片中提取2000个category-independent region proposals (通过selective search方法)。
* 由于选取出来的proposals大小不同，考虑到后续的CNN要求输入图片的大小一致，所以要对proposal进行resize操作（为了避免图像扭曲严重，中间可以采取一些技巧减少图像扭曲）。
* 通过CNN后，extract a fixed-length feature vector from each proposal.
* 然后通过category-specific linear SVMs对每个region分类。

![R-CNN](/public/img/posts/R-CNN/1515627113170.png)

> A second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training, followed by supervised fine-tuning (e.g., [29]). The second major contribution of this paper is to show that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain-specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce. In our experiments, fine-tuning for detection improves mAP performance by 8 percentage points. After fine-tuning, our system achieves a mAP of 54% on VOC 2010 compared to 33% for the highly-tuned, HOGbased deformable part model (DPM) [14, 17].
* 在detection中的第二个问题是labeled data的缺乏，没有足够的数据去训练 a large CNN. The conventional solution to this problem is to use unsupervised pre-training, followed by supervised fine-tuning.
* 本文的第二个贡献就是给出了另一种方法：**supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain-specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce.**

> Our system is also quite efficient. The only class-specific computations are a reasonably small matrix-vector product and greedy non-maximum suppression. This computational property follows from features that are shared across all categories and that are also two orders of magnitude lowerdimensional than previously used region features (cf. [32]).

---

### Object detection with R-CNN
> Our object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs. In this section, we present our design decisions for each module, describe their test-time usage, detail how their parameters are learned, and show results on PASCAL VOC 2010-12.
目标识别系统包含three module:
* The first module generates ***category-independent*** region proposals. 这些proposals就是要输入到detector的候选区域。
* The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. 第二个module是一个CNN网络，从每个region提取一个fixed-length feature vector。
* The third module is a set of class-specific linear SVMs. 第三部分就是一个线性的SVM分类器。

#### Module design
**Region proposals**
> While R-CNN is agnostic to the particular region proposal method, we use selective search to enable a controlled comparison with prior detection work.
* 用selective search选择region proposal，可以和prior detection work进行比较。

**Feature extraction**
> We extract a 4096-dimensional feature vector from each region proposal using the Caffe [21] implementation of the CNN described by Krizhevsky et al. [22]. Features are computed by forward propagating a mean-subtracted 227  227 RGB image through five convolutional layers and two fully connected layers. We refer readers to [21, 22] for more network architecture details.

> In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatible with the CNN (its architecture requires inputs of a fixed 227  227 pixel size). Of the many possible transformations of our arbitrary-shaped regions, we opt for the simplest. Regardless of the size or aspect ratio of the candidate region, we warp all pixels in a tight bounding box around it to the required size. Prior to warping, we dilate the tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original box (we use p = 16). Figure 2 shows a random sampling of warped training regions. The supplementary material discusses alternatives to warping.
* 首先要把image data转换成与CNN网络结构compatible的输入形式；
* The supplementary material discusses alternatives to warping.

#### Test-time detection
> At test time, we run selective search on the test image to extract around 2000 region proposals (we use selective search’s “fast mode” in all experiments). We warp each proposal and forward propagate it through the CNN in order to read off features from the desired layer. Then, for each class, we score each extracted feature vector using the SVM trained for that class. **Given all scored regions in an image, we apply a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection-over-union (IoU) overlap with a higher scoring selected region larger than a learned threshold.**
* Non-maximum suppression (for each class independently)[非极大抑制1](http://blog.csdn.net/danieljianfeng/article/details/43084875) [非极大抑制2](http://blog.csdn.net/pb09013037/article/details/45477591)
**Run-time analysis**
> Two properties make detection efficient. First, all CNN parameters are shared across all categories. Second, the feature vectors computed by the CNN are low-dimensional when compared to other common approaches, such as spatial pyramids with bag-of-visual-word encodings. The features used in the UVA detection system [32], for example, are two orders of magnitude larger than ours (360k vs. 4k-dimensional).
有两个properties make detection efficient:
* All CNN parameters are shared across all categories, 所有CNN参数在所有类别中共享；
* The feature vectors computed by the CNN are low-dimensional when compared to other common approaches, feature vector的维度小 (4k)，计算量小很多。

> The result of such sharing is that the time spent computing region proposals and features (13s/image on a GPU or 53s/image on a CPU) is amortized over all classes. The only class-specific computations are dot products between features and SVM weights and non-maximum suppression. In practice, all dot products for an image are batched into a single matrix-matrix product. The feature matrix is typically 20004096 and the SVM weight matrix is 4096N, where N is the number of classes.
* 计算region proposals和features的时间是amortized (分摊) over all classes.
* The only class-specific computaions are dot products between **features** and **SVM weights** and **non-maximum supression**.
* 关于图像的dot products 可以分批为 a single matrix-matrix product.

#### Training
**Supervised pre-training**
> We discriminatively pre-trained the CNN on a large auxiliary dataset (ILSVRC 2012) with image-level  annotations (i.e., no bounding box labels).
* 有区别的pre-train, 在a large auxiliary dataset, 只有图片的层面的识别，没有bounding box的label。

**Domain-specific fine-tuning**
> To adapt our CNN to the new task (detection) and the new domain (warped VOC windows), we continue stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals from VOC. Aside from replacing the CNN’s ImageNet-specific 1000-way classification layer with a randomly initialized 21-way classification layer (for the 20 VOC classes plus background), the CNN architecture is unchanged.
* 为了CNN网络可以适用于新的task (detection)和新的数据(warped VOC windows)，继续用stochastic gradient descent (SGD) 方法训练CNN参数，using only warped region proposals from VOC.
* 除了更改classification layer外，其他的CNN结构不变。
> We treat all region proposals with  0:5 IoU overlap with a ground-truth box as positives for that box’s class and the rest as negatives. We start SGD at a learning rate of 0.001 (1/10th of the initial pre-training rate), which allows fine-tuning to make progress while not clobbering the initialization. In each SGD iteration, we uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128. We bias the sampling towards positive windows because they are extremely rare compared to background.

**Object category classifiers**
> Consider training a binary classifier to detect cars. It’s clear that an image region tightly enclosing a car should be a positive example. Similarly, it’s clear that a background region, which has nothing to do with cars, should be a negative example. **Less clear is how to label a region that partially overlaps a car. We resolve this issue with an IoU overlap threshold, below which regions are defined as negatives.** The overlap threshold, $0.3$, was selected by a grid search over ${0, 0.1, ..., 0.5}$ on a validation set. We found that selecting this threshold carefully is important.
* 如果一个bounding box紧紧包含一个car，将其标记为positive label；相应的就将背景标记为negative label。
* 如果一个bounding box只包含car的一部分，则标记就没有那么容易了。此时引入IoU overlap threshold，低于这个threshold的region就标记为negative label。***threshold选择很重要***

> Once features are extracted and training labels are applied, we optimize one linear SVM per class. Since the training data is too large to fit in memory, we adopt the standard hard negative mining method [14, 30]. Hard negative mining converges quickly and in practice mAP stops increasing after only a single pass over all images.
* 由于training data太大，memory不一定够用，采用standard hard negative mining method，因为converges quickly，即收敛很快。

#### Results on PASCAL VOC 2010-12

---

### Visualization, ablation, and modes of error
#### Visualizing learned features *(need one more read)*

#### Ablation studies
##### Performance layer-by-layer, without fine-tuning
> To understand which layers are critical for detection performance, we analyzed results on the VOC 2007 dataset for each of the CNN’s last three layers. 
> > Layer $fc_{6}$ is fully connected to $pool_{5}$. To compute features, it multiplies a $4096\times9216$ weight matrix by the $pool_{5}$ feature map (reshaped as a 9216-dimensional vector) and then adds a vector of biases. This intermediate vector is component-wise half-wave rectified $(x\leftarrow max(0, x))$.
> > Layer $fc_{7}$ is the final layer of the network. It is implemented by multiplying the features computed by $fc_{6}$ by a $4096\times9216$  weight matrix, and similarly adding a vector of biases and applying half-wave rectification.

#### Performance layer-by-layer, with fine-tuning
> We now look at results from our CNN after having fine-tuned its parameters on VOC 2007 trainval. The improvement is striking (Table 2 rows 4-6): fine-tuning increases mAP by 8.0 percentage points to 54.2%. The boost from fine-tuning is much larger for $fc_{6}$ and $fc_{7}$ than for $pool_{5}$, which suggests that the pool5 features learned from ImageNet are general and that most of the improvement is gained from learning domain-specific non-linear classifiers on top of them.
* fine-tuned对performance的改进效果显著， increases mAP by 8.0 percentage points to $54.2\%$. 

#### Comparison to recent feature learning methods
> Relatively few feature learning methods have been tried on PASCAL VOC detection. We look at two recent approaches that build on deformable part models. For reference, we also include results for the standard HOG-based DPM [17].
> * The first DPM feature learning method, DPM ST [25], augments HOG features with histograms of “sketch token” probabilities.
> * The second method, DPM HSC [27], replaces HOG with histograms of sparse codes (HSC).

#### Detection error analysis
> We applied the excellent detection analysis tool from Hoiem et al. [20] in order to reveal our method’s error modes, understand how fine-tuning changes them, and to see how our error types compare with DPM.
* 用一种tool分析实验结果，具体的工具也没说是什么，不在本文讨论范围内。

#### Bounding box regression
> Based on the error analysis, we implemented a simple method to reduce localization errors. Inspired by the bounding box regression employed in DPM [14], we train a linear regression model to predict a new detection window given the $pool_{5}$ features for a selective search region proposal.
* 基于error analysis，用一个简单的simple method to reduce errors, inspired by the **bounding box regression**.

---

### Semantic segmentation
> Region classification is a standard technique for semantic segmentation, allowing us to easily apply R-CNN to the PASCAL VOC segmentation challenge.

**CNN features for segmentation**
> We evaluate three strategies for computing features on CPMC regions, all of which begin by warping the rectangular window around the region to $227\times 227$.
* $full$
* $gt$
* $full+gt$

**Results on VOC 2011**

---

### Conclusion
> In recent years, object detection performance had stagnated. The best performing systems were complex ensembles combining multiple low-level image features with high-level context from object detectors and scene classifiers. This paper presents a simple and scalable object detection algorithm that gives a 30% relative improvement over the best previous results on PASCAL VOC 2012. We achieved this performance through two insights. **The first is to apply high-capacity convolutional neural networks to bottom-up region proposals in order to localize and segment objects.**  **The second is a paradigm for training large CNNs when labeled training data is scarce.** We show that it is highly effective to pre-train the network— with supervision—for a auxiliary task with abundant data (image classification) and then to fine-tune the network for the target task where data is scarce (detection). We conjecture that the “supervised pre-training/domain-specific finetuning” paradigm will be highly effective for a variety of data-scarce vision problems. We conclude by noting that it is significant that we achieved these results by using a combination of classical tools from computer vision and deep learning (bottomup region proposals and convolutional neural networks). Rather than opposing lines of scientific inquiry, the two are natural and inevitable partners.
* Two insights:
	* high-capacity convolutional neural nerworks.
	* labeled training data is scarce. 
* **"supervied pre-training/domain-specific fine-tuning"** paradigm will be highly effective for a variety of data-scarce vision problems.

> We conclude by noting that it is significant that we achieved these results by using a combination of **classical tools from computer vision** and **deep learning *(bottom-up region proposals and convolutional neural networks)***. Rather than opposing lines of scientific inquiry, the two are natural and inevitable partners.